ML 1

#Importing the required libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn import metrics

data = pd.read_csv(r'C:\Users\Aastha\Downloads\uber.csv')

data

data.head()

data.tail()

data.info()

data.describe()

data.shape

data.columns

data = data.drop(['Unnamed: 0', 'key'], axis = 1)

data.shape

data

data['month'] = data['pickup_datetime']
data

data['month'] = data['month'].str.slice(start = 5, stop = 7)
data

data['hour'] = data['pickup_datetime']
data

data['hour'] = data['hour'].str.slice(start = 11, stop = 13)
data

data = data.drop(['pickup_datetime'], axis = 1)
data

data.describe()

data.info()

data["dropoff_longitude"]= data["dropoff_longitude"].fillna(data['dropoff_longitude'].mean())
data["dropoff_latitude"]= data["dropoff_latitude"].fillna(data['dropoff_latitude'].mean())

# data.dropna(inplace=True)------- Drop complete row

data.info()

def haversine (lon_1, lon_2, lat_1, lat_2):
    lon_1, lon_2, lat_1, lat_2 = map(np.radians, [lon_1, lon_2, lat_1, lat_2])
    diff_lon = lon_2 - lon_1
    diff_lat = lat_2 - lat_1
    km = 2 * 6371* np.arcsin(np.sqrt(np.sin(diff_lat/2.0)**2+np.cos(lat_1)* np.cos(lat_2)*np.sin(diff_lon/2.0)**2))
    return km

data['distance'] = haversine(data['pickup_longitude'],data['dropoff_longitude'],data['pickup_latitude'],data['dropoff_latitude'])

data.describe()

data.replace(to_replace = 0, value = data['passenger_count'].mean(), inplace=True)
data.replace(to_replace = 0, value = data['distance'].mean(), inplace=True)
data[data['fare_amount'] <= 0] = data['fare_amount'].mean()

data.describe()

data.info()

data

data.plot(kind = "box",subplots = True, layout = (6,2),figsize=(15,20))

# Using the InterQuartile Range to fill the values
def remove_outlier(df1 , col):
    Q1 = df1[col].quantile(0.25)
    Q3 = df1[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_whisker = Q1 - 1.5 * IQR
    upper_whisker = Q3 + 1.5 * IQR
    data[col] = np.clip(df1[col] , lower_whisker , upper_whisker)
    return df1

def treat_outliers_all(df1 , col_list):
    for c in col_list:
        df1 = remove_outlier(data , c)
    return df1

cols = ['fare_amount', 'pickup_longitude', 'pickup_latitude',
       'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'distance']
data = treat_outliers_all(data , cols)

data.plot(kind = "box",subplots = True, layout = (6,2),figsize=(15,20))

data.shape

data.isnull().sum()

import seaborn as sns
import matplotlib.pyplot as plt

plt.scatter(data['pickup_longitude'], data['fare_amount'])
plt.xlabel("pickup_longitude")
plt.ylabel("fare_amount")

plt.scatter(data['pickup_latitude'], data['fare_amount'])
plt.xlabel("pickup_latitude")
plt.ylabel("fare_amount")

plt.scatter(data['dropoff_longitude'], data['fare_amount'])
plt.xlabel("dropoff_longitude")
plt.ylabel("fare_amount")

plt.scatter(data['dropoff_longitude'], data['fare_amount'])
plt.xlabel("dropoff_longitude")
plt.ylabel("fare_amount")

plt.scatter(data['distance'], data['fare_amount'])
plt.xlabel("distance")
plt.ylabel("fare_amount")

corr_matrix = data.corr()
corr_matrix

X = data.iloc[:, 1:]
y = data.iloc[:, 0]
print(X.shape, y.shape)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)
print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)

from sklearn.linear_model import LinearRegression
linear_regression = LinearRegression().fit(X_train, y_train)

linear_regression.score(X_test, y_test)

y_pred = linear_regression.predict(X_test)

result = pd.DataFrame()
result['Actual'], result['Predicted'] = y_test, y_pred
result.sample(10)

print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
print('R Squared (R²):', np.sqrt(metrics.r2_score(y_test, y_pred)))

from sklearn.ensemble import RandomForestRegressor
random_forest = RandomForestRegressor(n_estimators = 10, random_state = 42)

random_forest.fit(X_train, y_train)

random_forest.score(X_test, y_test)

y_pred = random_forest.predict(X_test)

result = pd.DataFrame()
result['Actual'], result['Predicted'] = y_test, y_pred
result.sample(10)

print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
print('R Squared (R²):', np.sqrt(metrics.r2_score(y_test, y_pred)))

